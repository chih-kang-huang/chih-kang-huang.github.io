[
  {
    "objectID": "old/index.html",
    "href": "old/index.html",
    "title": "My Academic Page",
    "section": "",
    "text": "About Research Publications Teaching Contact"
  },
  {
    "objectID": "old/index.html#about",
    "href": "old/index.html#about",
    "title": "My Academic Page",
    "section": "About Me",
    "text": "About Me\nI am a researcher focusing on machine learning and computational modeling‚Ä¶"
  },
  {
    "objectID": "old/index.html#research",
    "href": "old/index.html#research",
    "title": "My Academic Page",
    "section": "Research",
    "text": "Research\nMy work explores neural networks, reproducibility, and structured learning‚Ä¶"
  },
  {
    "objectID": "old/index.html#publications",
    "href": "old/index.html#publications",
    "title": "My Academic Page",
    "section": "Publications",
    "text": "Publications\n\nDoe, J. (2025). Everforest architectures and interpretability.\n\nDoe, J. (2024). Reproducible MultiH5Datasets in AI research."
  },
  {
    "objectID": "old/index.html#teaching",
    "href": "old/index.html#teaching",
    "title": "My Academic Page",
    "section": "Teaching",
    "text": "Teaching\nI teach graduate-level courses on deep learning and model interpretability."
  },
  {
    "objectID": "old/index.html#contact-contact",
    "href": "old/index.html#contact-contact",
    "title": "My Academic Page",
    "section": "Contact #contact",
    "text": "Contact #contact\nüìß you@example.com\nüåê yourwebsite.com\n\n\n\n\n¬© 2025 Your Name"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chih-Kang Huang",
    "section": "",
    "text": "Research Publications Projects Misc"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Chih-Kang Huang",
    "section": "üëã About Me",
    "text": "üëã About Me\n\n\n\n\n\n\nPostdoctoral Researcher | PhD in Applied Maths | ENS Lyon\n\n\nI am a postdoctoral researcher specializing in physics-informed machine learning and control theory of PDEs at Institut Elie Cartan de Lorrane and Institut Jean Lamour. My current work focuses on modeling, simulation, and optimization in materials science, with keen interests in quantum computing and quantitative finance.\nPreviously, I received my Ph.D.¬†in Applied Mathematics at Institut Camille Jordan, University of Lyon. My manuscript can be found here.\n\n\n GitHub ‚Ä¢  LinkedIn ‚Ä¢  Email\n\n\n\n\n\nüì£ News\n\n[2025/07] Invited talk of our work at TMS2026!"
  },
  {
    "objectID": "index.html#research",
    "href": "index.html#research",
    "title": "Chih-Kang Huang",
    "section": "üî¨ Research",
    "text": "üî¨ Research\n\n\nControl Theory of Deep learning\nAnalysis of PDEs and Calculus of variations\nApplication of AI to Materials Science\nQuantum Simulations and Control\n\n\n\nüõ†Ô∏è Skills\n\n\nProgramming : Python (Pandas, scikit-learn, JAX, Pytorch), MatLab\nTools : Linux, Latex, Git, Docker, Slurm (HPC), JupyterLab"
  },
  {
    "objectID": "index.html#works",
    "href": "index.html#works",
    "title": "Chih-Kang Huang",
    "section": "üìú Publications and Preprints",
    "text": "üìú Publications and Preprints\n\n\n\nC-K. Huang, L. Gagnon, B. Appolaire, M. Zaloznik. Neural network approximation of a phase-field model for dendritic growth.\nE.Bretin, C-K. Huang, S. Masnou. A thickness-aware Allen-Cahn equation for the mean curvature flow of thin structures, accepted.\nC-K. Huang. Questions of approximation and compactness for geometric variational problems, Thesis, 2021\nC-K. Huang. Asymptotic Analysis of embedded Willmore spheres in 3-dimensional manifolds.\nL. Gagnon, C-K. Huang. Null controllability of Allen-Cahn eqaution, in preparation.\n\n\n\n\nüó£Ô∏è Presentations (Click to Expand)\n\n\nUnder construction"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Chih-Kang Huang",
    "section": "üî® Projects",
    "text": "üî® Projects\n\n\nContributions to Variational Quantum Brushes using VQE and quantum control, Quantinuum Hackathon, Bradford 2025\nDelivered lectures on Physics-Informed Machine Learning at the International PEPR DIADEM Summer School, Paris 2025\nChatbot for automated meal order collection, a chatbot to collect automatically meal orders during my mandatory civil service in Taiwan\n\n\nüí° Hackathons\n\nIBM Quantum Computing Hackathon, Quantum framework for dynamic portfolio optimization, London 2025\nG-Research Paris Quant Challenge 2024, Introductory session on cointegration for pairs trading.\nCENTURI Hackathon 2024 for Quantitative Biology, Deep single molecule unfolding detection, First Price."
  },
  {
    "objectID": "index.html#misc",
    "href": "index.html#misc",
    "title": "Chih-Kang Huang",
    "section": "‚ô¶Ô∏è Misc",
    "text": "‚ô¶Ô∏è Misc\n\nIn my spare time, i enjoy studying and playing Bridge, a probabilistic trick-taking card game. My system based on Precision Club and Symmetric Relay can be found here.\n\n\n\nüìö Teaching (Click to Expand)\n\n\nUnder construction \n\n\n\nResearch Publications Projects Misc\n\n\n\n¬© 2025 Chih-Kang Huang. All rights reserved."
  },
  {
    "objectID": "backup/markdown_generator/readme.html",
    "href": "backup/markdown_generator/readme.html",
    "title": "Jupyter notebook markdown generator",
    "section": "",
    "text": "Jupyter notebook markdown generator\nThese .ipynb files are Jupyter notebook files that convert a TSV containing structured data about talks (talks.tsv) or presentations (presentations.tsv) into individual markdown files that will be properly formatted for the academicpages template. The notebooks contain a lot of documentation about the process. The .py files are pure python that do the same things if they are executed in a terminal, they just don‚Äôt have pretty documentation."
  },
  {
    "objectID": "backup/files/PINNs_black-scholes.html",
    "href": "backup/files/PINNs_black-scholes.html",
    "title": "PINNs applied to simplified Black-Scholes model",
    "section": "",
    "text": "Given the simplified Black-Scholes model : for \\((x, t) \\in [0, 1]^2\\), we have \\[\n\\partial_t u + x\\partial_x u + \\frac{x^2}{2} \\partial^2_{x^2} u - u = 0\n\\] with initial condition: \\(u(x, 0) = e^x - 1\\)\nHere, we demonstrate how to implement PINNs to solve the Black-Scholes model\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nimport numpy as np\n\n\n\"\"\"\nIn our case: #inputs = 2 (x,t)\nand #outputs = 1\n\n\"\"\"\n\nclass PINN(nn.Module):\n    def __init__(self, n_hidden, n_layers):\n        \"\"\"\n        n_hidden = nb of hyperparameters in each hidden layers \n        n_layers = nb of hidden layers \n        \"\"\"\n        super(PINN, self).__init__()\n        self.layers = nn.ModuleList() \n        for l in range(n_layers): \n            if l == 0 : \n                self.layers.append(nn.Linear(2, n_hidden))\n                self.layers.append(nn.Sigmoid())\n            else:\n                self.layers.append(nn.Linear(n_hidden, n_hidden))\n                self.layers.append(nn.Sigmoid())\n        self.layers.append(nn.Linear(n_hidden, 1))\n\n    def forward(self, x,t):\n        inputs = torch.cat([x,t],axis=1) \n        return nn.Sequential(*self.layers)(inputs)\n\n\n### (2) Model\npinn = PINN(8, 4)\npinn = pinn.to(device) # to GPU if available\nmse_cost_function = torch.nn.MSELoss() # Mean squared error\noptimizer = torch.optim.Adam(pinn.parameters())\n\n\n## PDE as loss function. Thus would use the network which we call as u_theta\ndef f(x,t, pinn):\n    u = pinn(x,t) # the dependent variable u is given by the network based on independent variables x,t\n    u_x = torch.autograd.grad(u.sum(), x, create_graph=True)[0]\n    u_xx = torch.autograd.grad(u_x.sum(), x, create_graph=True)[0]\n    u_t = torch.autograd.grad(u.sum(), t, create_graph=True)[0]\n    pde = u_t + x*u_x + (x**2/2)* u_xx - u\n    return pde\n\n\n## Data from Boundary Conditions\n# u(x,0)= e^x -1\n## BC just gives us datapoints for training\n\n# BC tells us that for any x in range[0,1] and time=0, the value of u is given by 6e^(-3x)\n# Take say 500 random numbers of x\nx_bc = np.random.uniform(low=0.0, high=1.0, size=(500,1))\nt_bc = np.zeros((500,1))\n# compute u based on BC\nu_bc = np.exp(x_bc) - 1\n\n\n### (3) Training / Fitting\niterations = 20000\nprevious_validation_loss = 99999999.0\nfor epoch in range(iterations):\n    \n    # Loss based on boundary conditions\n    pt_x_bc = Variable(torch.from_numpy(x_bc).float(), requires_grad=False).to(device)\n    pt_t_bc = Variable(torch.from_numpy(t_bc).float(), requires_grad=False).to(device)\n    pt_u_bc = Variable(torch.from_numpy(u_bc).float(), requires_grad=False).to(device)\n    \n    net_bc_out = pinn(pt_x_bc, pt_t_bc) # output of u(x,t)\n    mse_u = mse_cost_function(net_bc_out, pt_u_bc)\n    \n    # Loss based on PDE\n    x_collocation = np.random.uniform(low=0.0, high=1.0, size=(500,1))\n    t_collocation = np.random.uniform(low=0.0, high=1.0, size=(500,1))\n    all_zeros = np.zeros((500,1))\n    \n    \n    pt_x_collocation = Variable(torch.from_numpy(x_collocation).float(), requires_grad=True).to(device)\n    pt_t_collocation = Variable(torch.from_numpy(t_collocation).float(), requires_grad=True).to(device)\n    pt_all_zeros = Variable(torch.from_numpy(all_zeros).float(), requires_grad=False).to(device)\n    \n    f_out = f(pt_x_collocation, pt_t_collocation, pinn) # output of f(x,t)\n    mse_f = mse_cost_function(f_out, pt_all_zeros)\n    \n    # Combining the loss functions\n    loss = mse_u + mse_f\n    \n    \n    optimizer.zero_grad() # to make the gradients zero\n    loss.backward() # This is for computing gradients using backward propagation\n    optimizer.step() # This is equivalent to : theta_new = theta_old - alpha * derivative of J w.r.t theta\n\n    if epoch % 500 == 0:\n        with torch.autograd.no_grad():\n            print(epoch,\"Traning Loss:\",loss.data)\n    \n\n0 Traning Loss: tensor(0.7658, device='cuda:0')\n500 Traning Loss: tensor(0.5181, device='cuda:0')\n1000 Traning Loss: tensor(0.0681, device='cuda:0')\n1500 Traning Loss: tensor(0.0059, device='cuda:0')\n2000 Traning Loss: tensor(0.0030, device='cuda:0')\n2500 Traning Loss: tensor(0.0016, device='cuda:0')\n3000 Traning Loss: tensor(0.0009, device='cuda:0')\n3500 Traning Loss: tensor(0.0007, device='cuda:0')\n4000 Traning Loss: tensor(0.0007, device='cuda:0')\n4500 Traning Loss: tensor(0.0005, device='cuda:0')\n5000 Traning Loss: tensor(0.0005, device='cuda:0')\n5500 Traning Loss: tensor(0.0004, device='cuda:0')\n6000 Traning Loss: tensor(0.0003, device='cuda:0')\n6500 Traning Loss: tensor(0.0002, device='cuda:0')\n7000 Traning Loss: tensor(0.0002, device='cuda:0')\n7500 Traning Loss: tensor(0.0001, device='cuda:0')\n8000 Traning Loss: tensor(0.0001, device='cuda:0')\n8500 Traning Loss: tensor(0.0001, device='cuda:0')\n9000 Traning Loss: tensor(8.5434e-05, device='cuda:0')\n9500 Traning Loss: tensor(7.9389e-05, device='cuda:0')\n10000 Traning Loss: tensor(6.8866e-05, device='cuda:0')\n10500 Traning Loss: tensor(6.7111e-05, device='cuda:0')\n11000 Traning Loss: tensor(5.4925e-05, device='cuda:0')\n11500 Traning Loss: tensor(5.5811e-05, device='cuda:0')\n12000 Traning Loss: tensor(4.8830e-05, device='cuda:0')\n12500 Traning Loss: tensor(4.1139e-05, device='cuda:0')\n13000 Traning Loss: tensor(3.5004e-05, device='cuda:0')\n13500 Traning Loss: tensor(3.5637e-05, device='cuda:0')\n14000 Traning Loss: tensor(2.7562e-05, device='cuda:0')\n14500 Traning Loss: tensor(2.6412e-05, device='cuda:0')\n15000 Traning Loss: tensor(2.9109e-05, device='cuda:0')\n15500 Traning Loss: tensor(2.1074e-05, device='cuda:0')\n16000 Traning Loss: tensor(2.0789e-05, device='cuda:0')\n16500 Traning Loss: tensor(2.0803e-05, device='cuda:0')\n17000 Traning Loss: tensor(2.8370e-05, device='cuda:0')\n17500 Traning Loss: tensor(1.8630e-05, device='cuda:0')\n18000 Traning Loss: tensor(1.8292e-05, device='cuda:0')\n18500 Traning Loss: tensor(2.3269e-05, device='cuda:0')\n19000 Traning Loss: tensor(1.9506e-05, device='cuda:0')\n19500 Traning Loss: tensor(1.7237e-05, device='cuda:0')\n\n\n\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter\nimport numpy as np\n\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')\n\nx=np.arange(0,1,0.02)\nt=np.arange(0,1,0.02)\nms_x, ms_t = np.meshgrid(x, t)\n\nx = np.ravel(ms_x).reshape(-1,1)\nt = np.ravel(ms_t).reshape(-1,1)\n\npt_x = Variable(torch.from_numpy(x).float(), requires_grad=True).to(device)\npt_t = Variable(torch.from_numpy(t).float(), requires_grad=True).to(device)\npt_u = pinn(pt_x,pt_t)\nu=pt_u.data.cpu().numpy()\nms_u = u.reshape(ms_x.shape)\n\nsurf = ax.plot_surface(ms_x,ms_t,ms_u, cmap=cm.coolwarm,linewidth=0, antialiased=False)\n             \nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n\nfig.colorbar(surf, shrink=0.5, aspect=5)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Save Model\n#torch.save(net.state_dict(), \"model_PINN.pt\")"
  },
  {
    "objectID": "backup/talkmap_out.html",
    "href": "backup/talkmap_out.html",
    "title": "Leaflet cluster map of talk locations",
    "section": "",
    "text": "Assuming you are working in a Linux or Windows Subsystem for Linux environment, you may need to install some dependencies. Assuming a clean installation, the following will be needed:\nsudo apt install jupyter\nsudo apt install python3-pip\npip install python-frontmatter getorg --upgrade\nAfter which you can run this from the _talks/ directory, via:\n jupyter nbconvert --to notebook --execute talkmap.ipynb --output talkmap_out.ipynb\nThe _talks/ directory contains .md files of all your talks. This scrapes the location YAML field from each .md file, geolocates it with geopy/Nominatim, and uses the getorg library to output data, HTML, and Javascript for a standalone cluster map.\n\n# Start by installing the dependencies\n!pip install python-frontmatter getorg --upgrade\nimport frontmatter\nimport glob\nimport getorg\nfrom geopy import Nominatim\nfrom geopy.exc import GeocoderTimedOut\n\nCollecting python-frontmatter\n  Downloading python_frontmatter-1.1.0-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: getorg in /opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages (0.3.1)\nRequirement already satisfied: PyYAML in /opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages (from python-frontmatter) (6.0.2)\nRequirement already satisfied: geopy in /opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages (from getorg) (2.4.1)\nRequirement already satisfied: pygithub in /opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages (from getorg) (2.6.1)\nRequirement already satisfied: retrying in /opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages (from getorg) (1.3.4)\nRequirement already satisfied: geographiclib&lt;3,&gt;=1.52 in /opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages (from geopy-&gt;getorg) (2.0)\nRequirement already satisfied: pynacl&gt;=1.4.0 in /opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages (from pygithub-&gt;getorg) (1.5.0)\nRequirement already satisfied: requests&gt;=2.14.0 in /opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages (from pygithub-&gt;getorg) (2.32.3)\nRequirement already satisfied: pyjwt&gt;=2.4.0 in /opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages (from pyjwt[crypto]&gt;=2.4.0-&gt;pygithub-&gt;getorg) (2.10.1)\nRequirement already satisfied: typing-extensions&gt;=4.0.0 in /opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages (from pygithub-&gt;getorg) (4.12.2)\nRequirement already satisfied: urllib3&gt;=1.26.0 in /opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages (from pygithub-&gt;getorg) (2.3.0)\nRequirement already satisfied: Deprecated in /opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages (from pygithub-&gt;getorg) (1.2.18)\nRequirement already satisfied: six&gt;=1.7.0 in /opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages (from retrying-&gt;getorg) (1.17.0)\nRequirement already satisfied: cryptography&gt;=3.4.0 in /opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages (from pyjwt[crypto]&gt;=2.4.0-&gt;pygithub-&gt;getorg) (44.0.1)\nRequirement already satisfied: cffi&gt;=1.4.1 in /opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages (from pynacl&gt;=1.4.0-&gt;pygithub-&gt;getorg) (1.17.1)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages (from requests&gt;=2.14.0-&gt;pygithub-&gt;getorg) (3.4.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages (from requests&gt;=2.14.0-&gt;pygithub-&gt;getorg) (3.10)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages (from requests&gt;=2.14.0-&gt;pygithub-&gt;getorg) (2025.1.31)\nRequirement already satisfied: wrapt&lt;2,&gt;=1.10 in /opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages (from Deprecated-&gt;pygithub-&gt;getorg) (1.17.2)\nRequirement already satisfied: pycparser in /opt/hostedtoolcache/Python/3.9.21/x64/lib/python3.9/site-packages (from cffi&gt;=1.4.1-&gt;pynacl&gt;=1.4.0-&gt;pygithub-&gt;getorg) (2.22)\nDownloading python_frontmatter-1.1.0-py3-none-any.whl (9.8 kB)\nInstalling collected packages: python-frontmatter\nSuccessfully installed python-frontmatter-1.1.0\nIywidgets and ipyleaflet support disabled. You must be in a Jupyter notebook to use this feature.\nError raised:\nNo module named 'ipyleaflet'\nCheck that you have enabled ipyleaflet in Jupyter with:\n    jupyter nbextension enable --py ipyleaflet\n\n\n\n# Collect the Markdown files\ng = glob.glob(\"_talks/*.md\")\n\n\n# Set the default timeout, in seconds\nTIMEOUT = 5\n\n# Prepare to geolocate\ngeocoder = Nominatim(user_agent=\"academicpages.github.io\")\nlocation_dict = {}\nlocation = \"\"\npermalink = \"\"\ntitle = \"\"\n\nIn the event that this times out with an error, double check to make sure that the location is can be properly geolocated.\n\n# Perform geolocation\nfor file in g:\n    # Read the file\n    data = frontmatter.load(file)\n    data = data.to_dict()\n\n    # Press on if the location is not present\n    if 'location' not in data:\n        continue\n\n    # Prepare the description\n    title = data['title'].strip()\n    venue = data['venue'].strip()\n    location = data['location'].strip()\n    description = f\"{title}&lt;br /&gt;{venue}; {location}\"\n\n    # Geocode the location and report the status\n    try:\n        location_dict[description] = geocoder.geocode(location, timeout=TIMEOUT)\n        print(description, location_dict[description])\n    except ValueError as ex:\n        print(f\"Error: geocode failed on input {location} with message {ex}\")\n    except GeocoderTimedOut as ex:\n        print(f\"Error: geocode timed out on input {location} with message {ex}\")\n    except Exception as ex:\n        print(f\"An unhandled exception occurred while processing input {location} with message {ex}\")\n\nTalk 2 on Relevant Topic in Your Field&lt;br /&gt;London School of Testing; London, UK London, Greater London, England, United Kingdom\nTutorial 1 on Relevant Topic in Your Field&lt;br /&gt;UC-Berkeley Institute for Testing Science; Berkeley, CA, USA Berkeley, Alameda County, California, United States\nConference Proceeding talk 3 on Relevant Topic in Your Field&lt;br /&gt;Testing Institute of America 2014 Annual Conference; Los Angeles, CA, USA Los Angeles, Los Angeles County, California, United States\nTalk 1 on Relevant Topic in Your Field&lt;br /&gt;UC San Francisco, Department of Testing; San Francisco, CA, USA San Francisco, California, United States\n\n\n\n# Save the map\nm = getorg.orgmap.create_map_obj()\ngetorg.orgmap.output_html_cluster_map(location_dict, folder_name=\"talkmap\", hashed_usernames=False)\n\n'Written map to talkmap/'"
  },
  {
    "objectID": "backup/CONTRIBUTING.html",
    "href": "backup/CONTRIBUTING.html",
    "title": "",
    "section": "",
    "text": "Contributions are welcome!\nPlease add issues and make pull requests. There are no stupid questions. All ideas are welcome. This is a volunteer project. Be excellent to each other.\nBug reports and feature requests to the template should be submitted via GitHub. For questions concerning how to style the template, please feel free to start a new discussion on GitHub.\nFork from master and go from there. Remember that this repository is intended to remain a generic, ready-to-fork template that demonstrates the features of academicpages."
  },
  {
    "objectID": "backup/files/real_estate_Chih-Kang_HUANG.html",
    "href": "backup/files/real_estate_Chih-Kang_HUANG.html",
    "title": "Test technique Yanport du 21 Ao√ªt 2023",
    "section": "",
    "text": "Chih-Kang HUANG"
  },
  {
    "objectID": "backup/files/real_estate_Chih-Kang_HUANG.html#objectif",
    "href": "backup/files/real_estate_Chih-Kang_HUANG.html#objectif",
    "title": "Test technique Yanport du 21 Ao√ªt 2023",
    "section": "Objectif :",
    "text": "Objectif :\nNous vous proposons de regrouper les annonces immobili√®res portant sur un m√™me bien immobilier. En effet, plusieurs professionnels (voire particuliers) publient sur plusieurs portails immobiliers des annonces portant sur le m√™me bien immobilier et il y a un int√©r√™t fort de les d√©doublonner pour rendre la recherche plus efficace mais aussi pour avoir des statistiques plus pertinentes.\nLa strat√©gie repose sur une m√©thode hybride, combinant des comparaisons classiques et une analyse de texte de base √† l‚Äôaide d‚Äôun mod√®le de langage.\nHypoth√®se : Dans ce document, nous supposons qu‚Äôil n‚Äôy a ni de fausses saisies ni de fausses annonces. Nous aborderons ce point de vue √† la conclusion de ce document et proposerons quelques pistes d‚Äôam√©lioration √©ventuelles.\nLes √©tapes cl√©s sont les suivantes :\n\nApr√®s avoir minutieusement analys√© et nettoy√© nos donn√©es, nous commen√ßons par identifier les caract√©ristiques essentielles qui permettent de d√©terminer si deux annonces sont tr√®s probablement identiques ou diff√©rentes. Nous classifions principalement ces caract√©ristiques en trois cat√©gories : liens, cat√©goriques et num√©riques.\nNous exploitons un mod√®le de langage avanc√© sur la colonne ‚ÄúDESCRIPTION‚Äù afin de quantifier les similitudes entre deux textes.\nEnfin, en utilisant les fonctions de crit√®res que nous avons √©labor√©es pr√©c√©demment, nous cr√©ons un nouveau DataFrame avec une colonne de cl√©s d‚Äôidentification. Cette cl√© nous permettra de regrouper facilement les annonces d√©crivant le m√™me bien immobilier.\n\n\n## Only for google Colab \n#!pip install huggingface_hub\n#!pip install transformers sentencepiece protobuf tokenizer\n#\n#from google.colab import drive\n#\n#drive.mount('/content/drive'\n#======================================================\n\n# Chargement des bilioth√®ques dont nous aurions besoin :\nimport pandas as pd\nimport numpy as np\n\n# helpful modules\nimport json # change string to list/dictionnary\nimport matplotlib.pyplot as plt\nimport seaborn as sns # data visualization\n\n# CrossEncoder pour le traitement automatique du langage\nfrom sentence_transformers import CrossEncoder\n\n# import fuzzywuzzy\n# from fuzzywuzzy import process\n# import charset_normalizer\n\n# Zero-shot Classification\n# from transformers import pipeline\n# from huggingface_hub import notebook_login\n# \n# notebook_login()\n# \n# classifier = pipeline(\"zero-shot-classification\",\n#                       model=\"joeddav/xlm-roberta-large-xnli\")\n#classifier = pipeline(\"zero-shot-classification\",\n#                      model=\"joeddav/xlm-roberta-large-xnli\", device=0) # GPU\n\nLors du chargement d‚Äôun fichier CSV en tant que DataFrame √† l‚Äôaide de Pandas, il est essentiel de prendre en compte les probl√®mes potentiels li√©s √† l‚Äôencodage. Cela se produit fr√©quemment dans les pays o√π les langues utilis√©es ne sont pas bas√©es sur l‚Äôalphabet standard :\n\ndata_path = \"./Dataset - Ads _ Levallois-Perret - 2019-08 - export-ads-levallois-perret-2019-08-27.csv\"\ndf = pd.read_csv(data_path, encoding=\"utf-8\")\ndf.head(10)\n\n\n\n\n\n\n\n\nID\nURL\nCRAWL_SOURCE\nPROPERTY_TYPE\nNEW_BUILD\nDESCRIPTION\nIMAGES\nSURFACE\nLAND_SURFACE\nBALCONY_SURFACE\n...\nDEALER_NAME\nDEALER_TYPE\nCITY_ID\nCITY\nZIP_CODE\nDEPT_CODE\nPUBLICATION_START_DATE\nPUBLICATION_END_DATE\nLAST_CRAWL_DATE\nLAST_PRICE_DECREASE_DATE\n\n\n\n\n0\n22c05930-0eb5-11e7-b53d-bbead8ba43fe\nhttp://www.avendrealouer.fr/location/levallois...\nA_VENDRE_A_LOUER\nAPARTMENT\nFalse\nAu rez de chauss√©e d'un bel immeuble r√©cent,ap...\n[\"https://cf-medias.avendrealouer.fr/image/_87...\n72.00\nNaN\nNaN\n...\nLamirand Et Associes\nAGENCY\n54178039\nLevallois-Perret\n92300.0\n92\n2017-03-22T04:07:56.095\nNaN\n2017-04-21T18:52:35.733\nNaN\n\n\n1\n8d092fa0-bb99-11e8-a7c9-852783b5a69d\nhttps://www.bienici.com/annonce/ag440414-16547...\nBIEN_ICI\nAPARTMENT\nFalse\nJe vous propose un appartement dans la rue Col...\n[\"http://photos.ubiflow.net/440414/165474561/p...\n48.00\nNaN\nNaN\n...\nProprietes Privees\nMANDATARY\n54178039\nLevallois-Perret\n92300.0\n92\n2018-09-18T11:04:44.461\nNaN\n2019-06-06T10:08:10.89\n2018-09-25\n\n\n2\n44b6a5c0-3466-11e9-8213-25cc7d9bf5fc\nhttps://www.bellesdemeures.com/annonces/vente/...\nBELLES_DEMEURES\nAPARTMENT\nFalse\nDans un cadre arbor√©, calme et fleuri, un pent...\n[\"https://v.seloger.com/s/width/965/visuels/0/...\n267.00\nNaN\nNaN\n...\nPropri√©t√©s Parisiennes\nAGENCY\n54178039\nLevallois-Perret\n92300.0\n92\n2019-02-19T16:49:03.547\nNaN\n2019-06-13T08:22:14.314\nNaN\n\n\n3\ne9e07ed0-812f-11e8-82aa-61eacebe4584\nhttps://www.seloger.com/annonces/locations/bur...\nSE_LOGER\nPREMISES\nFalse\n\"Le meilleur coworking flexible de la ville, 5...\n[\"https://pix.yanport.com/ads/e9e07ed0-812f-11...\n50.00\nNaN\nNaN\n...\nIwg\nAGENCY\n54178039\nLevallois-Perret\n92300.0\n92\n2018-07-06T15:18:59.805\nNaN\n2019-06-18T10:40:07.405\nNaN\n\n\n4\n872302b0-5a21-11e9-950c-510fefc1ed35\nhttps://www.bellesdemeures.com/annonces/vente/...\nBELLES_DEMEURES\nHOUSE\nFalse\nLevallois - Parc de la Planchette A toute prox...\n[\"https://v.seloger.com/s/width/966/visuels/0/...\n330.00\nNaN\nNaN\n...\nDaniel Feau Neuilly\nAGENCY\n54178039\nLevallois-Perret\n92300.0\n92\n2019-04-08T17:12:20.123\nNaN\n2019-06-14T15:02:18.155\n2019-06-14\n\n\n5\nde04afa0-2e5f-11e8-bda5-3334b193df7f\nhttps://immobilier.lefigaro.fr/annonces/annonc...\nEXPLORIMMO\nAPARTMENT\nFalse\nLEVALLOIS - FRONT DE SEINE - 5-6 PIECES Appart...\n[\"http://photo3.pericles.fr/photo_get.php?S1=3...\n142.76\nNaN\nNaN\n...\nBuilding Partners\nAGENCY\n54178039\nLevallois-Perret\n92300.0\n92\n2018-03-23T06:03:10.963\nNaN\n2019-07-10T15:06:39.615\nNaN\n\n\n6\n530c7ac0-36a5-11e7-a435-b3bf21527190\nhttps://www.seloger.com/annonces/locations/bur...\nSE_LOGER\nPREMISES\nFalse\nLEVALLOIS, Paul Vaillant Couturier, pr√®s gare ...\n[\"https://v.seloger.com/s/cdn/x/visuels/2/2/r/...\n63.00\nNaN\nNaN\n...\nCabinet De L''ouest Parisien\nAGENCY\n54178039\nLevallois-Perret\n92300.0\n92\n2017-04-27T01:35:00.674\nNaN\n2019-07-10T15:06:17.12\n2019-07-10\n\n\n7\n5135e0a0-62dd-11e8-91b3-8d7b603ef3b8\nhttps://immobilier.lefigaro.fr/annonces/annonc...\nEXPLORIMMO\nPARKING\nNaN\nLouez au mois un parking priv√© au 2 rue Mauric...\n[\"http://photos.ubiflow.net/751982/151820943/p...\n16.00\nNaN\nNaN\n...\nYespark\nAGENCY\n54178039\nLevallois-Perret\n92300.0\n92\n2018-05-29T01:12:11.485\nNaN\n2019-07-10T02:46:51.652\nNaN\n\n\n8\nbc076370-45ec-11e9-a00a-897941d6ca9c\nhttps://www.leboncoin.fr/locations/1583612210.htm\nLE_BON_COIN\nPARKING\nNaN\nLOUE place de parking dans r√©sidence de standi...\n[\"https://pix.yanport.com/ads/bc076370-45ec-11...\n11.00\nNaN\nNaN\n...\nbaudoin\nPRIVATE\n54178039\nLevallois-Perret\n92300.0\n92\n2019-03-14T00:04:24.651\nNaN\n2019-07-13T11:03:51.699\nNaN\n\n\n9\n08491e50-8dfe-11e9-8a42-a16638394d93\nhttps://www.leboncoin.fr/ventes_immobilieres/1...\nLE_BON_COIN\nAPARTMENT\nFalse\nA VENDRE APPARTEMENT DE 2 PI√àCES MAIRIE DE LEV...\n[\"https://pix.yanport.com/ads/08491e50-8dfe-11...\n57.00\nNaN\nNaN\n...\nImpact Immo\nAGENCY\n54178039\nLevallois-Perret\n92300.0\n92\n2019-06-13T17:09:20.737\nNaN\n2019-07-13T18:17:01.782\n2019-07-13\n\n\n\n\n10 rows √ó 57 columns\n\n\n\nNous observons dans le DataFrame que certaines cellules ne sont pas remplies, affichant plut√¥t la valeur NaN\n\na, b  = df.shape\nmissing_value_count = df.isnull().sum()\nnb_empty_cases = missing_value_count.sum()/(a*b)\nprint (\"La taille des donn√©es est de\", df.shape, \"\\n il y a {} de cellules non remplies\".format(missing_value_count.sum()), \", soit {}% des cases non remplis\".format(100* nb_empty_cases))\n\nLa taille des donn√©es est de (2164, 57) \n il y a 64386 de cellules non remplies , soit 52.19865745695107% des cases non remplis\n\n\n\ndf.dtypes.unique()\n\narray([dtype('O'), dtype('float64'), dtype('bool'), dtype('int64')],\n      dtype=object)\n\n\n\ndf.nunique()\n\nID                            2164\nURL                           2164\nCRAWL_SOURCE                    13\nPROPERTY_TYPE                    5\nNEW_BUILD                        2\nDESCRIPTION                   1914\nIMAGES                        1947\nSURFACE                        454\nLAND_SURFACE                     2\nBALCONY_SURFACE                  0\nTERRACE_SURFACE                 18\nROOM_COUNT                      11\nBEDROOM_COUNT                    7\nBATHROOM_COUNT                   0\nLUNCHROOM_COUNT                  0\nTOILET_COUNT                     0\nFURNISHED                        2\nFIREPLACE                        0\nAIR_CONDITIONING                 0\nGARDEN                           1\nSWIMMING_POOL                    1\nBALCONY                          0\nTERRACE                          1\nCELLAR                           0\nPARKING                          2\nPARKING_COUNT                    8\nHEATING_TYPES                    5\nHEATING_MODE                     3\nFLOOR                           12\nFLOOR_COUNT                     16\nCONSTRUCTION_YEAR               48\nELEVATOR                         2\nCARETAKER                        2\nENERGY_CONSUMPTION               0\nGREENHOUSE_GAS_CONSUMPTION       0\nMARKETING_TYPE                   2\nPRICE                          764\nPRICE_M2                       997\nPRICE_EVENTS                  1733\nRENTAL_EXPENSES                129\nRENTAL_EXPENSES_INCLUDED         2\nDEPOSIT                         44\nFEES                            71\nFEES_INCLUDED                    2\nEXCLUSIVE_MANDATE                2\nAGENCIES_UNWANTED                2\nOCCUPIED                         2\nDEALER_NAME                    553\nDEALER_TYPE                      3\nCITY_ID                          1\nCITY                             1\nZIP_CODE                         1\nDEPT_CODE                        1\nPUBLICATION_START_DATE        2103\nPUBLICATION_END_DATE             0\nLAST_CRAWL_DATE               2164\nLAST_PRICE_DECREASE_DATE       145\ndtype: int64\n\n\n\ndf.shape\n\n(2164, 57)"
  },
  {
    "objectID": "backup/files/real_estate_Chih-Kang_HUANG.html#traitement-des-donn√©es",
    "href": "backup/files/real_estate_Chih-Kang_HUANG.html#traitement-des-donn√©es",
    "title": "Test technique Yanport du 21 Ao√ªt 2023",
    "section": "Traitement des donn√©es",
    "text": "Traitement des donn√©es\nCommencer par les √©tapes essentielles du traitement des donn√©es (Data Processing), en proc√©dant aux √©tapes suivantes :\n\n√âliminer les colonnes non remplies ou ayant peu d‚Äôutilit√©.\nIdentifier les colonnes de type liste/dictionnaire et les colonnes num√©riques.\nRemplir les valeurs manquantes (par exemple, FEES -&gt; FEES_INCLUDED, RENTAL -&gt; INCLUDED) et assurer la coh√©rence entre SURFACE, PRICE, PRICE_M2, PRICE_EVENT, et CRAWL_DATE.\nCompl√©ter les valeurs manquantes dans la colonne DESCRIPTION.\n\n\n1. √âlimination des colonnes peu utiles\nCommen√ßons par supprimer les colonnes qui ne contiennent aucune donn√©e :\n\nuseless_cols = [cname for cname in df.columns if df[cname].nunique() ==0]\n\nainsi que la colonne ‚ÄúCRAWL_SOURCE‚Äù :\n\nX = df.copy().drop(useless_cols, axis =1).drop([\"CRAWL_SOURCE\"], axis=1)\n\nLes colonnes qui ne contiennent qu‚Äôune seule valeur, telles que CITY, CITY_ID, ZIP_CODE, sont g√©n√©ralement peu informatives. Le m√™me constat s‚Äôapplique aux autres colonnes qui affichent √©galement une unique valeur :\n\none_value_cols = [cname for cname in X.columns if X[cname].nunique() ==1]\nprint(one_value_cols)\n\n['GARDEN', 'SWIMMING_POOL', 'TERRACE', 'CITY_ID', 'CITY', 'ZIP_CODE', 'DEPT_CODE']\n\n\nDans le cadre de notre √©tude, nous avons pris la d√©cision de laisser de c√¥t√© ces colonnes, m√™me si certaines d‚Äôentre elles pourraient √©ventuellement √™tre utiles en relation avec les colonnes images (bien que nous n‚Äôenvisagions pas non plus de proc√©der √† la visualisation des photos, comme cela sera expliqu√© ult√©rieurement).\n\nX = X.drop(one_value_cols, axis=1)\nX.shape\n\n(2164, 38)\n\n\n\n\n2. Identification des colonnes de type liste et Rectification des types\nIl n‚Äôy a pas beaucoup de colonnes de type liste, nous pouvons les extraire en utilisant le module json.loads. Elles comprennent :\n\n[\"URL\", \"IMAGES\", \"HEATING_TYPE\", \"PRICE_EVENTS\"]\n\n['URL', 'IMAGES', 'HEATING_TYPE', 'PRICE_EVENTS']\n\n\nCertaines colonnes num√©riques, telles que ‚ÄúFLOOR‚Äù, ‚ÄúCONSTRUCTION_YEAR‚Äù, ‚ÄúPARKING_COUNT‚Äù, etc., sont en r√©alit√© plut√¥t cat√©goriques. Nous avons tendance √† les traiter comme des cha√Ænes de caract√®res au lieu de les modifier. Nous corrigeons donc leur type :\n\nX[\"CONSTRUCTION_YEAR\"] = X[\"CONSTRUCTION_YEAR\"].astype('object')\nX[\"FLOOR\"] = X[\"FLOOR\"].astype('object')\nfor cname in X.columns:\n    if \"COUNT\" in cname:\n        X[cname] = X[cname].astype('object')\n\n\n\n3. Coh√©rences entre certaines colonnes\n\nRENTAL_EXPENSES, FEES\n\nNous d√©finissons ‚ÄúTrue‚Äù pour RENTAL_EXPENSES_INCLUDED et ‚ÄúFEES_INCLUDED‚Äù si le montant est effectivement indiqu√© dans les colonnes correspondantes :\n\nfor i in X.index :\n    if pd.notnull(X.loc[i, \"RENTAL_EXPENSES\"]):\n        X.loc[i, \"RENTAL_EXPENSES_INCLUDED\"] = True\n    if pd.notnull(X.loc[i, \"FEES\"]):\n        X.loc[i, \"FEES_INCLUDED\"] = True\n\nExemple : FEES_INCLUDED √©tait √† NaN alors que la valeur de FEES √©tait bien renseign√©e pour l‚Äôannonce 115 :\n\nX.loc[115, \"FEES\"], X.loc[115, \"FEES_INCLUDED\"]\n\n(823.5, True)\n\n\n\nSURFACE, PRICE, PRICE_M2, PRICE_EVENTS et DATES\n\nLes colonnes ‚ÄúSURFACE‚Äù, ‚ÄúPRICE‚Äù et ‚ÄúPRICE_M2‚Äù de certaines entr√©es sont soit bien remplies, soit pr√©sentent un grand nombre de valeurs manquantes, ce qui rend leur restitution difficile :\n\ns = []\nfor i in X.index:\n    k = pd.isnull(X.loc[i, \"SURFACE\"])+ pd.isnull(X.loc[i, \"PRICE\"]) + pd.isnull(X.loc[i, \"PRICE_M2\"])\n    if k == 1:\n        s.append(str(k))\nprint(s)\n\n[]\n\n\nHeuresuement, les champs PRICE_EVENTS sont d√ªrement remplies :\n\nX[\"PRICE_EVENTS\"].isnull().any()\n\nFalse\n\n\n\n\nCoh√©rence entre PRICE et PRICE_EVENTS:\nLes prix indiqu√©s dans ‚ÄúPRICE‚Äù sont-ils correctement enregistr√©s dans ‚ÄúPRICE_EVENTS‚Äù ?\n\nX[\"PRICE\"].isnull().sum()\n\n60\n\n\nNous identifions les annonces dont les valeurs de ‚ÄúPRICE‚Äù ne sont pas enregistr√©es dans ‚ÄúPRICE_EVENTS‚Äù.\n\nprice_not_in_event = []\nprice_good = 0\nfor i in X.index:\n    price = X.loc[i, \"PRICE\"]\n    price_events = json.loads(X.loc[i, \"PRICE_EVENTS\"]) # list of dictionaries\n    price_list = [k[\"price\"] for k in price_events]\n    if pd.isnull(price) or (price in price_list):\n        pass\n    else:\n      price_not_in_event.append(i)\nprint(price_not_in_event)\n\n[65, 1113, 1258]\n\n\nNous pla√ßons ici les prix dans ‚ÄúPRICE_EVENTS‚Äù en tenant compte de la ‚ÄúLAST_CRAWL_DATE‚Äù.\n\nX.loc[price_not_in_event, [\"PRICE\", \"PRICE_EVENTS\", ]]\n\n\n\n\n\n\n\n\nPRICE\nPRICE_EVENTS\n\n\n\n\n65\n48893.0\n[{\"price\":48868.0,\"date\":\"2019-07-25\"}]\n\n\n1113\n29269.0\n[{\"price\":72058.0,\"date\":\"2018-01-11\"},{\"price...\n\n\n1258\n1227200.0\n[{\"price\":1180000.0,\"date\":\"2019-07-30\"},{\"pri...\n\n\n\n\n\n\n\n\nCRAWLDATE = pd.to_datetime(X[\"LAST_CRAWL_DATE\"], format='mixed').dt.date\n\n\nCRAWLDATE[0].strftime(\"%Y-%m-%d\")\n\n'2017-04-21'\n\n\n\ncorrect_price_events = []\nfor i in price_not_in_event :\n    price = X.loc[i, \"PRICE\"]\n    crawl_date = CRAWLDATE[i].strftime(\"%Y-%m-%d\")\n    price_dict = {\"price\": price,\"date\":crawl_date}\n    event_list = json.loads(X.loc[i, \"PRICE_EVENTS\"]) + [price_dict]\n    correct_price_events += [event_list]\n\ncorrect_price_events[0]\n\n[{'price': 48868.0, 'date': '2019-07-25'},\n {'price': 48893.0, 'date': '2019-08-14'}]\n\n\n\nfor i in range(len(price_not_in_event)):\n    X.loc[price_not_in_event[i], \"PRICE_EVENTS\"] = json.dumps(correct_price_events[i])\n\n√âtant donn√© que nous nous assurons que la colonne ‚ÄúPRICE_EVENTS‚Äù contient les informations les plus r√©centes, certaines colonnes deviennent redondantes et sont donc retir√©es de notre √©tude :\n\nX = X.drop([\"PUBLICATION_START_DATE\", \"LAST_CRAWL_DATE\", \"PRICE\", \"LAST_PRICE_DECREASE_DATE\"], axis=1)\n\n\n\n4. DESCRIPTION\nOn remplit les cellules vides avec fillna:\n\nX['DESCRIPTION'].isnull().sum()\n\n4\n\n\n\nX['DESCRIPTION'] = X['DESCRIPTION'].fillna('0')"
  },
  {
    "objectID": "backup/files/real_estate_Chih-Kang_HUANG.html#classification-des-colonnes-caract√©ristiques-et-de-leur-fiabilit√©",
    "href": "backup/files/real_estate_Chih-Kang_HUANG.html#classification-des-colonnes-caract√©ristiques-et-de-leur-fiabilit√©",
    "title": "Test technique Yanport du 21 Ao√ªt 2023",
    "section": "Classification des colonnes caract√©ristiques et de leur fiabilit√©",
    "text": "Classification des colonnes caract√©ristiques et de leur fiabilit√©\nDans cette section, nous allons discuter des colonnes caract√©ristiques, de leurs types et de leur fiabilit√©. Commen√ßons par les conna√Ætre :\n\nX.columns\n\nIndex(['ID', 'URL', 'PROPERTY_TYPE', 'NEW_BUILD', 'DESCRIPTION', 'IMAGES',\n       'SURFACE', 'LAND_SURFACE', 'TERRACE_SURFACE', 'ROOM_COUNT',\n       'BEDROOM_COUNT', 'FURNISHED', 'PARKING', 'PARKING_COUNT',\n       'HEATING_TYPES', 'HEATING_MODE', 'FLOOR', 'FLOOR_COUNT',\n       'CONSTRUCTION_YEAR', 'ELEVATOR', 'CARETAKER', 'MARKETING_TYPE',\n       'PRICE_M2', 'PRICE_EVENTS', 'RENTAL_EXPENSES',\n       'RENTAL_EXPENSES_INCLUDED', 'DEPOSIT', 'FEES', 'FEES_INCLUDED',\n       'EXCLUSIVE_MANDATE', 'AGENCIES_UNWANTED', 'OCCUPIED', 'DEALER_NAME',\n       'DEALER_TYPE'],\n      dtype='object')\n\n\n\nlen(X.columns)\n\n34\n\n\n\nX.dtypes\n\nID                           object\nURL                          object\nPROPERTY_TYPE                object\nNEW_BUILD                    object\nDESCRIPTION                  object\nIMAGES                       object\nSURFACE                     float64\nLAND_SURFACE                float64\nTERRACE_SURFACE             float64\nROOM_COUNT                   object\nBEDROOM_COUNT                object\nFURNISHED                    object\nPARKING                        bool\nPARKING_COUNT                object\nHEATING_TYPES                object\nHEATING_MODE                 object\nFLOOR                        object\nFLOOR_COUNT                  object\nCONSTRUCTION_YEAR            object\nELEVATOR                     object\nCARETAKER                    object\nMARKETING_TYPE               object\nPRICE_M2                    float64\nPRICE_EVENTS                 object\nRENTAL_EXPENSES             float64\nRENTAL_EXPENSES_INCLUDED     object\nDEPOSIT                     float64\nFEES                        float64\nFEES_INCLUDED                object\nEXCLUSIVE_MANDATE              bool\nAGENCIES_UNWANTED            object\nOCCUPIED                     object\nDEALER_NAME                  object\nDEALER_TYPE                  object\ndtype: object\n\n\n\nColonnes ‚ÄúID‚Äù, ‚ÄúURL‚Äù et ‚ÄúIMAGES‚Äù :\n\n‚ÄúID‚Äù : Nous utilisons ‚ÄúID‚Äù pour g√©n√©rer les cl√©s de regroupement. Nous v√©rifions si chaque annonce a un ‚ÄúID‚Äù diff√©rent.\n‚ÄúURL‚Äù : De nature similaire √† ‚ÄúID‚Äù.\n\n\nX[\"ID\"].nunique() == len(X.index),  X[\"URL\"].nunique() == len(X.index)\n\n(True, True)\n\n\n\nid_cols = [\"ID\", \"URL\"]\n\n\n‚ÄúIMAGES‚Äù : Il est tr√®s probable que deux annonces se rapportent au m√™me bien immobilier si leurs liens ‚ÄúIMAGES‚Äù ont une intersection non nulle.\n\nCependant, certaines annonces ne contiennent qu‚Äôune image de type publicit√©, par exemple :\n\nimage_cols = [\"IMAGES\"]\n\n\nX.loc[26, \"IMAGES\"], X.loc[687, \"IMAGES\"]\n\n('[\"https://v.seloger.com/s/cdn/x/visuels/1/r/y/p/1ryphs9d2l7xv8w1s80a8t7csl6d49ed62tw60lic.jpg\"]',\n '[\"https://v.seloger.com/s/cdn/x/visuels/1/r/y/p/1ryphs9d2l7xv8w1s80a8t7csl6d49ed62tw60lic.jpg\"]')\n\n\n\n\nColonne Description :\nNous allons utiliser un mod√®le de langage pour traiter les descriptions des annonces. Nous d√©taillerons cela ult√©rieurement :\n\ndesc_cols = [\"DESCRIPTION\"]\n\n\n\nColonnes Cat√©goriques :\n\nColonnes Bool√©ennes : Certaines colonnes sont de nature bool√©enne, et nous pouvons comparer les valeurs associ√©es une par une :\n\n\nbool_cols = [cname for cname in X.columns if (X[cname].nunique() == 2) and (X[cname].dtype in ['bool', 'boolean', 'object'])]\nprint(bool_cols)\nprint(len(bool_cols))\n\n['NEW_BUILD', 'FURNISHED', 'PARKING', 'ELEVATOR', 'CARETAKER', 'MARKETING_TYPE', 'RENTAL_EXPENSES_INCLUDED', 'FEES_INCLUDED', 'EXCLUSIVE_MANDATE', 'AGENCIES_UNWANTED', 'OCCUPIED']\n11\n\n\nContre-exemple : Dans un premier coup, il semblait que ce sont de bons indicateurs pour d√©terminer si deux annonces sont associ√©es √† diff√©rents biens immobiliers. Malheureusement, nous avons rep√©r√© certaines anomalies, par exemple, dans la colonne ‚ÄúEXCLUSIVE_MANDATE‚Äù :\n\nX.loc[[58, 762], \"EXCLUSIVE_MANDATE\"]\n\n58      True\n762    False\nName: EXCLUSIVE_MANDATE, dtype: bool\n\n\nCependant, en examinant d‚Äôautres colonnes et leurs descriptions, il est apparu que les annonces 58 et 762 sont presque identiques, y compris leurs descriptions‚Ä¶\n\nprint(X.loc[[58, 762], :].nunique()), \nprint(X.loc[ [58, 762], \"DESCRIPTION\"])\n\nID                          2\nURL                         2\nPROPERTY_TYPE               1\nNEW_BUILD                   1\nDESCRIPTION                 2\nIMAGES                      1\nSURFACE                     1\nLAND_SURFACE                0\nTERRACE_SURFACE             0\nROOM_COUNT                  1\nBEDROOM_COUNT               1\nFURNISHED                   0\nPARKING                     1\nPARKING_COUNT               0\nHEATING_TYPES               1\nHEATING_MODE                1\nFLOOR                       1\nFLOOR_COUNT                 1\nCONSTRUCTION_YEAR           1\nELEVATOR                    1\nCARETAKER                   1\nMARKETING_TYPE              1\nPRICE_M2                    1\nPRICE_EVENTS                2\nRENTAL_EXPENSES             0\nRENTAL_EXPENSES_INCLUDED    0\nDEPOSIT                     0\nFEES                        0\nFEES_INCLUDED               1\nEXCLUSIVE_MANDATE           2\nAGENCIES_UNWANTED           0\nOCCUPIED                    1\nDEALER_NAME                 1\nDEALER_TYPE                 1\ndtype: int64\n58     LEVALLOIS / GREFFULHE Dans un immeuble r√©cent ...\n762    LEVALLOIS / GREFFULHE Dans un immeuble r√©cent ...\nName: DESCRIPTION, dtype: object\n\n\n\nColonnes de type liste : Nous extrairons les valeurs de ces colonnes en utilisant json.loads :\n\n\nlist_cols = [\"HEATING_TYPES\", \"PRICE_EVENTS\"]\n\n\nColonnes de type objet :\n\n\nobject_cols = [cname for cname in X.columns if (cname not in bool_cols + list_cols + id_cols + image_cols + desc_cols) and (X[cname].dtype in ['object'])]\nprint(len(object_cols), object_cols)\n\n10 ['PROPERTY_TYPE', 'ROOM_COUNT', 'BEDROOM_COUNT', 'PARKING_COUNT', 'HEATING_MODE', 'FLOOR', 'FLOOR_COUNT', 'CONSTRUCTION_YEAR', 'DEALER_NAME', 'DEALER_TYPE']\n\n\n\ncategorical_cols = bool_cols + object_cols\n\n\n\nColonnes Num√©riques :\n\nnumeric_cols  = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\nprint(len(numeric_cols), numeric_cols)\n\n7 ['SURFACE', 'LAND_SURFACE', 'TERRACE_SURFACE', 'PRICE_M2', 'RENTAL_EXPENSES', 'DEPOSIT', 'FEES']\n\n\n\n\nRegroupement des colonnes caract√©ristiques :\n\nprint(len(X.columns), X.columns)\n\n34 Index(['ID', 'URL', 'PROPERTY_TYPE', 'NEW_BUILD', 'DESCRIPTION', 'IMAGES',\n       'SURFACE', 'LAND_SURFACE', 'TERRACE_SURFACE', 'ROOM_COUNT',\n       'BEDROOM_COUNT', 'FURNISHED', 'PARKING', 'PARKING_COUNT',\n       'HEATING_TYPES', 'HEATING_MODE', 'FLOOR', 'FLOOR_COUNT',\n       'CONSTRUCTION_YEAR', 'ELEVATOR', 'CARETAKER', 'MARKETING_TYPE',\n       'PRICE_M2', 'PRICE_EVENTS', 'RENTAL_EXPENSES',\n       'RENTAL_EXPENSES_INCLUDED', 'DEPOSIT', 'FEES', 'FEES_INCLUDED',\n       'EXCLUSIVE_MANDATE', 'AGENCIES_UNWANTED', 'OCCUPIED', 'DEALER_NAME',\n       'DEALER_TYPE'],\n      dtype='object')\n\n\n\ncategorical_cols = bool_cols + object_cols \n\nplt.title(\"Colonnes Caract√©ristiques\")\nplt.ylabel(\"Nb de colonnes\")\nsns.barplot(x= ['Image', 'List', 'Description', 'Categorical', 'Numeric'], y = [len(image_cols), len(list_cols), len(desc_cols), len(categorical_cols), len(numeric_cols) ], )"
  },
  {
    "objectID": "backup/files/real_estate_Chih-Kang_HUANG.html#similarit√©s-entre-les-annonces-immobili√®res",
    "href": "backup/files/real_estate_Chih-Kang_HUANG.html#similarit√©s-entre-les-annonces-immobili√®res",
    "title": "Test technique Yanport du 21 Ao√ªt 2023",
    "section": "Similarit√©s entre les annonces immobili√®res :",
    "text": "Similarit√©s entre les annonces immobili√®res :\n\nComparaison entre les liens images :\nNous consid√©rons que deux annonces sont associ√©es au m√™me bien immobilier si :\n\n‚ÄúIMAGES‚Äù : Leurs liens images ont une intersection non nulle, et en plus, nous imposons qu‚Äôils partagent au moins 2 liens en commun pour √©viter le cas des photos publicitaires (voir l‚Äôexemple des annonces 26 et 687).\n\nRemarques : Puisque certains liens d‚Äôimages, par exemple ceux du LE BON COIN 14, ne sont plus accessibles, nous nous contentons de comparer les liens au lieu de visualiser leurs photos contenues.\n\ndef common_links(a, b):\n    a_set = set(a)\n    b_set = set(b)\n    if len(a_set.intersection(b_set))&gt;=2:\n        return True\n    else:\n        return False\n\n\ndef compare_image(df, icols, i, j):\n    for k in icols:\n        i_links = json.loads(df.loc[i, k])\n        j_links = json.loads(df.loc[j, k])\n        return common_links(i_links, j_links)\n    \n\nExemples :\n\ncompare_image(X, image_cols, 26, 687), compare_image(X, image_cols, 3, 85)\n\n(False, True)\n\n\n\noutput = []\nfor j in X.index:\n    if compare_image(X, image_cols, 3, j):\n        output.append(j)\n\nprint(output)\n\n[3, 85, 493, 1119, 1548, 1845, 1866]\n\n\n\njson.loads(X.loc[3, \"IMAGES\"])\n\n['https://pix.yanport.com/ads/e9e07ed0-812f-11e8-82aa-61eacebe4584/image_fc7d9181f6042f4a630c7fdbc9f40ba3.jpg',\n 'https://v.seloger.com/s/width/800/visuels/1/j/4/5/1j4582b04i58fwekzdtgiu20uqw3a640e7w3papd4.jpg',\n 'https://v.seloger.com/s/width/800/visuels/1/y/c/d/1ycdcnohlrp0qttlt6edgfs4cjajfz8nmo02ox7fs.jpg',\n 'https://v.seloger.com/s/width/800/visuels/1/p/0/1/1p01zgt2uxdey4rb322p2lk8iq17l1c8w1ukk6bjc.jpg',\n 'https://v.seloger.com/s/width/800/visuels/0/g/d/o/0gdod5pizzrepy7g3gi4h4aq4b34ycr2lq6do10bs.jpg',\n 'https://v.seloger.com/s/width/800/visuels/2/0/o/g/20ogztaombstv4wfenyhqgj3r906mur2vbbvp70jw.jpg']\n\n\n\njson.loads(X.loc[85, \"IMAGES\"])\n\n['https://pix.yanport.com/ads/ac3439f0-812f-11e8-82aa-61eacebe4584/image_685502e001f0730593f66ff91b4f1c1a.jpg',\n 'https://v.seloger.com/s/width/800/visuels/1/p/0/1/1p01zgt2uxdey4rb322p2lk8iq17l1c8w1ukk6bjc.jpg',\n 'https://v.seloger.com/s/width/800/visuels/1/y/c/d/1ycdcnohlrp0qttlt6edgfs4cjajfz8nmo02ox7fs.jpg',\n 'https://v.seloger.com/s/width/800/visuels/0/g/d/o/0gdod5pizzrepy7g3gi4h4aq4b34ycr2lq6do10bs.jpg',\n 'https://v.seloger.com/s/width/800/visuels/1/n/1/y/1n1ycskv7p348kabig6p2bbnz4kwo3umpscyhdr6w.jpg',\n 'https://v.seloger.com/s/width/800/visuels/1/j/4/5/1j4582b04i58fwekzdtgiu20uqw3a640e7w3papd4.jpg',\n 'https://v.seloger.com/s/width/800/visuels/2/0/o/g/20ogztaombstv4wfenyhqgj3r906mur2vbbvp70jw.jpg']\n\n\n\n\nComparaison des textes des descriptions :\nNous utilisons un mod√®le de langage de base SentenceTransformer. Tout d‚Äôabord, nous calculons les longueurs maximales des descriptions :\n\nmax_length =0\nfor i in X.index:\n    desc = X.loc[i, \"DESCRIPTION\"]\n    max_length = max(max_length, len(desc))\nprint(s)\n\nmodel = CrossEncoder('dangvantuan/CrossEncoder-camembert-large', max_length=max_length)\n\n[]\n\n\nLimit√© par le mod√®le et la puissance de mon propre ordinateur portable, nous choisissons ici de prendre en compte seulement des 800 premiers caract√®res pour estimer les similarit√©s entre deux textes.\n\ndef compare_desc(df, dcols, i, j):\n    df[\"DESCRIPTION\"].astype('string')\n    i_desc = str(df.loc[i, \"DESCRIPTION\"]).lower()[0:800]\n    j_desc = str(df.loc[j, \"DESCRIPTION\"]).lower()[0:800]\n    return  (model.predict([i_desc, j_desc]))\n\nLes exemples avec les annonces (26, 687) et (58, 762) :\n\ncompare_desc(X, desc_cols, 26, 687), compare_desc(X, desc_cols, 58, 762)\n\n(0.43317997, 0.9946484)\n\n\n\n\nComparaison des colonnes cat√©goriques :\nNous consid√©rons que deux annonces ne sont pas associ√©es au m√™me bien immobilier s‚Äôil y a plus de deux valeurs distinctes dans les colonnes cat√©goriques.\n\ndef compare_cat(df, ccols, i, j): \n    i_list = df.loc[i, ccols].to_list()\n    j_list = df.loc[j, ccols].to_list()\n    result =0\n    k = 0 # compteur des comparaisons valables\n    while (result &lt;= 1) and (k&lt;len(ccols)):\n        i_k = i_list[k]\n        j_k = j_list[k]\n        if not pd.isnull(i_k) and not pd.isnull(j_k):\n            result += (i_k != j_k)\n        k += 1\n\n    return (result &lt;=1)\n\nEncore les fameux exemples avec (26, 687) et (58, 762) :\n\ncompare_cat(X, categorical_cols, 26, 687), compare_cat(X, categorical_cols, 58, 762),\n\n(False, True)\n\n\n\n\nComparaison des colonnes num√©riques et listes\n\nprint(list_cols, numeric_cols)\n\n['HEATING_TYPES', 'PRICE_EVENTS'] ['SURFACE', 'LAND_SURFACE', 'TERRACE_SURFACE', 'PRICE_M2', 'RENTAL_EXPENSES', 'DEPOSIT', 'FEES']\n\n\n\ndef compare_list(df, lcols, i, j):\n    i_heat = set(json.loads(df.loc[i, \"HEATING_TYPES\"]))\n    j_heat = set(json.loads(df.loc[j, \"HEATING_TYPES\"]))\n    if not (i_heat.issubset(j_heat) or j_heat.issubset(i_heat)):\n        return False\n    else: \n        i_price_ev= json.loads(df.loc[i, \"PRICE_EVENTS\"])\n        j_price_ev = json.loads(df.loc[j, \"PRICE_EVENTS\"])\n\n        i_price = i_price_ev[-1]['price']\n        j_price = j_price_ev[-1]['price']\n        return (abs(i_price - j_price) &lt;5)\n\n\ncompare_list(X, list_cols, 26, 687), compare_cat(X, list_cols, 58, 762),\n\n(False, True)\n\n\n\ndef compare_num(df, ncols, i, j):\n    criteria = 0 \n    k = 0 # compteur des comparisons valables\n\n    # SURFACE\n    i_surf = df.loc[i, \"SURFACE\"]\n    j_surf = df.loc[j, \"SURFACE\"]\n    if pd.notnull(i_surf) and pd.notnull(j_surf):\n        criteria += (abs(i_surf - j_surf) &lt;= 1)\n        k+= 1\n\n    # LAND SURFACE \n    i_land = df.loc[i, \"LAND_SURFACE\"]\n    j_land = df.loc[j, \"LAND_SURFACE\"]\n    if pd.notnull(i_land) and pd.notnull(j_land):\n        criteria += (abs(i_land - j_land) &lt;= 1)\n        k+= 1\n\n    # TERRACE SURFACE\n    i_terrace = df.loc[i, \"TERRACE_SURFACE\"]\n    j_terrace = df.loc[j, \"TERRACE_SURFACE\"]\n    if pd.notnull(i_terrace) and pd.notnull(j_terrace):\n        criteria += (abs(i_terrace - j_terrace) &lt;= 1)\n        k+= 1\n\n    # PRICE M2\n    i_pricem2 = df.loc[i, \"PRICE_M2\"]\n    j_pricem2 = df.loc[j, \"PRICE_M2\"]\n    if pd.notnull(i_pricem2) and pd.notnull(j_pricem2):\n        criteria += (abs(i_pricem2 - j_pricem2) &lt;= 5)\n        k+= 1\n    \n    # RENTAL_EXPENSES\n    i_rent = df.loc[i, \"RENTAL_EXPENSES\"]\n    j_rent = df.loc[j, \"RENTAL_EXPENSES\"]\n    if pd.notnull(i_rent) and pd.notnull(j_rent):\n        criteria += (abs(i_rent - j_rent) &lt;= 5)\n        k+= 1\n\n\n    # FEES\n    i_fees = df.loc[i, \"FEES\"]\n    j_fees = df.loc[j, \"FEES\"]\n    if pd.notnull(i_fees) and pd.notnull(j_fees):\n        criteria += (abs(i_fees - j_fees) &lt;= 5)\n        k+= 1\n\n    # DEPOSIT \n    i_deposit = df.loc[i, \"DEPOSIT\"]\n    j_deposit = df.loc[j, \"DEPOSIT\"]\n    if pd.notnull(i_deposit) and pd.notnull(j_deposit):\n        criteria += (abs(i_deposit - j_deposit) &lt;= 5)\n        k+= 1\n\n\n    return (criteria &gt; (k/2)) # retourner true si la moiti√© de comparaisons valables sont true\n    \n\nExemples :\n\ncompare_num(X, numeric_cols, 26, 687), compare_num(X, numeric_cols, 58, 762),\n\n(False, True)"
  },
  {
    "objectID": "backup/files/real_estate_Chih-Kang_HUANG.html#cr√©ation-des-cl√©s-de-regroupement",
    "href": "backup/files/real_estate_Chih-Kang_HUANG.html#cr√©ation-des-cl√©s-de-regroupement",
    "title": "Test technique Yanport du 21 Ao√ªt 2023",
    "section": "Cr√©ation des cl√©s de regroupement",
    "text": "Cr√©ation des cl√©s de regroupement\nNous disposons maintenant de toutes les fonctions de crit√®res/comparaisons et nous sommes en mesure de regrouper les annonces qui se rapportent au m√™me bien immobilier.\n\ndef same_real_estate(df, icols, dcols, ccols, lcols, ncols, i, j):\n    if compare_image(df, icols, i, j):\n        return True\n    elif compare_cat(df, ccols, i, j): \n        if compare_list(df, lcols, i, j) and compare_num(df, ncols, i, j):\n            return (compare_desc(df, dcols, i, j) &gt;= 0.85)\n    else:\n        return False\n\nNous soulignons que l‚Äôon compare les descriptions texte seulement au moment o√π nous n‚Äôarrivons pas √† distincter deux annonces avec les autres colonnes caract√©ristiques. Nous mettre le seuil 0.85 pour dire comme quoi 2 annonces sont associ√©es au m√™me bien immobilier, mais ce choix du seuil n‚Äôest pas impartial. Nous revenons √† ce seuil √† la fin, et nous discuterons des possibilit√©s pour rendre ce choix plus neutre et objectif.\n\nsame_real_estate(X, image_cols, desc_cols, categorical_cols, list_cols, numeric_cols, 3, 85)\n\nTrue\n\n\n\n\nsame_real_estate(X, image_cols, desc_cols, categorical_cols, list_cols, numeric_cols, 26, 687)\n\nFalse\n\n\n\nsame_real_estate(X, image_cols, desc_cols, categorical_cols, list_cols, numeric_cols, 58, 762)\n\nTrue\n\n\nEnfin, nous ex√©cutons une boucle for : pour chaque annonce, si la cl√© n‚Äôa pas encore √©t√© attribu√©e, nous utilisons son ‚ÄúID‚Äù comme cl√© et examinons si les annonces ult√©rieures sont associ√©e au m√™me bien immobilier.\n\ndef df_with_keys(df = X, icols = image_cols, dcols = desc_cols, ccols = categorical_cols, lcols = list_cols, ncols = numeric_cols): \n    Y = df.copy()\n    Y['KEY'] = pd.Series(dtype='object')\n    for i in Y.index : \n        if pd.isnull(Y.loc[i, 'KEY']):\n            Y.loc[i, 'KEY'] = Y.loc[i, 'ID']\n            for j in range(i+1, len(Y.index)) :\n                if same_real_estate(df, icols, dcols, ccols, lcols, ncols, i, j):\n                    Y.loc[j, 'KEY'] = Y.loc[i, 'KEY']\n    return Y\n\n        \n\nMise en action !\n\nX_with_keys = df_with_keys(X)\nprint(X_with_keys)\n# X_with_keys[\"KEY\"].nunique()\n\n                                        ID  \\\n0     22c05930-0eb5-11e7-b53d-bbead8ba43fe   \n1     8d092fa0-bb99-11e8-a7c9-852783b5a69d   \n2     44b6a5c0-3466-11e9-8213-25cc7d9bf5fc   \n3     e9e07ed0-812f-11e8-82aa-61eacebe4584   \n4     872302b0-5a21-11e9-950c-510fefc1ed35   \n...                                    ...   \n2159  d3579370-824f-11e9-af18-9742751bcff8   \n2160  cce3fc60-c86b-11e9-a6b2-651beb16710e   \n2161  beec50b0-c85e-11e9-92d3-cb429fb9e457   \n2162  8cba88c0-a07a-11e9-a8e6-0de7b497e456   \n2163  f3ae8be0-9fcf-11e9-ab3e-47ec2b68d334   \n\n                                                    URL PROPERTY_TYPE  \\\n0     http://www.avendrealouer.fr/location/levallois...     APARTMENT   \n1     https://www.bienici.com/annonce/ag440414-16547...     APARTMENT   \n2     https://www.bellesdemeures.com/annonces/vente/...     APARTMENT   \n3     https://www.seloger.com/annonces/locations/bur...      PREMISES   \n4     https://www.bellesdemeures.com/annonces/vente/...         HOUSE   \n...                                                 ...           ...   \n2159  https://www.bienici.com/annonce/ag750861-20035...     APARTMENT   \n2160  https://www.meilleursagents.com/annonces/locat...     APARTMENT   \n2161  https://www.leboncoin.fr/ventes_immobilieres/1...     APARTMENT   \n2162  https://www.logic-immo.com/detail-location-bae...       PARKING   \n2163  https://immobilier.lefigaro.fr/annonces/annonc...     APARTMENT   \n\n     NEW_BUILD                                        DESCRIPTION  \\\n0        False  Au rez de chauss√©e d'un bel immeuble r√©cent,ap...   \n1        False  Je vous propose un appartement dans la rue Col...   \n2        False  Dans un cadre arbor√©, calme et fleuri, un pent...   \n3        False  \"Le meilleur coworking flexible de la ville, 5...   \n4        False  Levallois - Parc de la Planchette A toute prox...   \n...        ...                                                ...   \n2159     False  LEVALLOIS-PERRET. Bel appartement id√©alement s...   \n2160     False  √Ä 300 m√®tres du M√©tro Pont de Levallois B√©con,...   \n2161     False  Levallois / Rue barbes - Dans un immeuble r√©ce...   \n2162       NaN  Levallois-Perret, rue Paul Vaillant Couturier,...   \n2163     False  2 pi√®ces de 40 m¬≤ compos√© d'un s√©jour avec ter...   \n\n                                                 IMAGES  SURFACE  \\\n0     [\"https://cf-medias.avendrealouer.fr/image/_87...     72.0   \n1     [\"http://photos.ubiflow.net/440414/165474561/p...     48.0   \n2     [\"https://v.seloger.com/s/width/965/visuels/0/...    267.0   \n3     [\"https://pix.yanport.com/ads/e9e07ed0-812f-11...     50.0   \n4     [\"https://v.seloger.com/s/width/966/visuels/0/...    330.0   \n...                                                 ...      ...   \n2159  [\"http://photos.ubiflow.net/750861/200354631/p...     50.0   \n2160  [\"http://thumbor.meilleursagents.com/j8Xioi19Y...     31.0   \n2161                                                 []     67.0   \n2162  [\"https://mmf.logic-immo.com/mmf/ads/photo-pro...      NaN   \n2163  [\"http://d1qfj231ug7wdu.cloudfront.net/picture...     40.0   \n\n      LAND_SURFACE  TERRACE_SURFACE ROOM_COUNT  ... RENTAL_EXPENSES_INCLUDED  \\\n0              NaN              NaN        3.0  ...                     True   \n1              NaN              NaN        2.0  ...                      NaN   \n2              NaN              NaN        6.0  ...                      NaN   \n3              NaN              NaN        0.0  ...                      NaN   \n4              NaN              NaN        8.0  ...                      NaN   \n...            ...              ...        ...  ...                      ...   \n2159           NaN              NaN        2.0  ...                      NaN   \n2160           NaN              NaN        2.0  ...                     True   \n2161           NaN              NaN        3.0  ...                      NaN   \n2162           NaN              NaN        NaN  ...                     True   \n2163           NaN              NaN        2.0  ...                      NaN   \n\n     DEPOSIT   FEES FEES_INCLUDED EXCLUSIVE_MANDATE AGENCIES_UNWANTED  \\\n0        NaN    NaN           NaN             False               NaN   \n1        NaN    NaN         False             False               NaN   \n2        NaN    NaN           NaN             False               NaN   \n3        NaN    NaN           NaN             False               NaN   \n4        NaN    NaN           NaN             False               NaN   \n...      ...    ...           ...               ...               ...   \n2159     NaN    NaN         False             False               NaN   \n2160     NaN  465.0          True             False               NaN   \n2161     NaN    NaN          True             False               NaN   \n2162     NaN    NaN           NaN              True               NaN   \n2163     NaN    NaN         False             False               NaN   \n\n     OCCUPIED                 DEALER_NAME DEALER_TYPE  \\\n0         NaN        Lamirand Et Associes      AGENCY   \n1       False          Proprietes Privees   MANDATARY   \n2       False      Propri√©t√©s Parisiennes      AGENCY   \n3         NaN                         Iwg      AGENCY   \n4       False         Daniel Feau Neuilly      AGENCY   \n...       ...                         ...         ...   \n2159    False          Haussmann Prestige      AGENCY   \n2160      NaN   MISTER PROPERTY LEVALLOIS      AGENCY   \n2161    False              Groupe Auteuil      AGENCY   \n2162      NaN  Nicolas Lanteri Immobilier      AGENCY   \n2163    False              Blg Immobilier      AGENCY   \n\n                                       KEY  \n0     22c05930-0eb5-11e7-b53d-bbead8ba43fe  \n1     8d092fa0-bb99-11e8-a7c9-852783b5a69d  \n2     44b6a5c0-3466-11e9-8213-25cc7d9bf5fc  \n3     e9e07ed0-812f-11e8-82aa-61eacebe4584  \n4     872302b0-5a21-11e9-950c-510fefc1ed35  \n...                                    ...  \n2159  86a5a800-81c3-11e9-a3ea-ffe50e0986b0  \n2160  538190f0-c8a5-11e9-92d3-cb429fb9e457  \n2161  ce8f07b0-a4a5-11e9-96d9-fb3253f8fc6c  \n2162  8cba88c0-a07a-11e9-a8e6-0de7b497e456  \n2163  31ab5210-9f1d-11e9-ab3e-47ec2b68d334  \n\n[2164 rows x 35 columns]\n\n\n\nX_with_keys[\"KEY\"].value_counts()\n\nKEY\nddc272a0-3991-11e9-8a1a-297fbcac27e2    43\n57baa230-c862-11e9-a6b2-651beb16710e     7\nbde25540-c4f1-11e9-a6b2-651beb16710e     7\n872302b0-5a21-11e9-950c-510fefc1ed35     7\n78e1ca10-35a5-11e9-8a1a-297fbcac27e2     7\n                                        ..\ne6511700-349b-11e9-8213-25cc7d9bf5fc     1\n6fc4cb10-8c6e-11e9-a7f2-f5443208fc78     1\n990b3520-ab51-11e9-aa5e-8b8909b4f047     1\nd04cd4f0-a28f-11e9-aa5e-8b8909b4f047     1\n8cba88c0-a07a-11e9-a8e6-0de7b497e456     1\nName: count, Length: 1484, dtype: int64"
  },
  {
    "objectID": "backup/files/real_estate_Chih-Kang_HUANG.html#sortie-des-donn√©es",
    "href": "backup/files/real_estate_Chih-Kang_HUANG.html#sortie-des-donn√©es",
    "title": "Test technique Yanport du 21 Ao√ªt 2023",
    "section": "Sortie des donn√©es",
    "text": "Sortie des donn√©es\n\ndf_with_keys = df.copy()\ndf_with_keys[\"KEY\"] = X_with_keys[\"KEY\"]\n\ndf_with_keys.to_csv('./submission.csv')"
  },
  {
    "objectID": "backup/files/real_estate_Chih-Kang_HUANG.html#piste-dam√©lioration",
    "href": "backup/files/real_estate_Chih-Kang_HUANG.html#piste-dam√©lioration",
    "title": "Test technique Yanport du 21 Ao√ªt 2023",
    "section": "Piste d‚Äôam√©lioration :",
    "text": "Piste d‚Äôam√©lioration :\n√Ä la fin de cette √©tude, nous proposons quelques pistes d‚Äôam√©lioration. Certaines n‚Äôont pas pu √™tre r√©alis√©es en raison du manque de temps ou de ressources disponibles :\n\nDans cette √©tude, en raison du manque de donn√©es suffisantes, nous avons utilis√© un mod√®le pr√©-entra√Æn√© pour √©valuer les similarit√©s entre les textes des descriptions. Nous avons choisi un seuil de 0,85 pour d√©terminer si deux descriptions sont similaires ou non. Une mani√®re de rendre ce choix plus rigoureux serait de r√©aliser une m√©thode de ‚Äúminmax‚Äù it√©ratif sur un sous-ensemble des donn√©es, plus concr√®tement,\n\n\n\nimport random\ndef seuil(df, n, p): \n# n = nb de fois d'iteration, p nb de sous-ensemble pris pour estimer le seuil\n    seuil_list = []\n    for k in range(n):\n        max_score = 0\n        L = random.sample(list(df.index), p)\n        for i in range(p):\n            for j in range(i+1, p):\n                max_score = max(max_score, compare_desc(df, desc_cols, i, j))\n        seuil_list.append(max_score)\n    return min(seuil_list)\n\nPar exemple, nous pourrions calculer le score maximal sur un ensemble de 50 donn√©es al√©atoires, puis prendre le minimum sur les 5 it√©rations :\n\nseuil(X, 5, 50)\n\n0.8465062\n\n\n\nCrit√®re de confiance : Il serait possible d‚Äôajouter une pond√©ration pour am√©liorer la pr√©cision de l‚Äôidentification. On pourrait s‚Äôattendre √† une combinaison lin√©aire ou non lin√©aire des fonctions comparatives. Si nous disposions de suffisamment de donn√©es d‚Äôentra√Ænement et de validation, nous pourrions tenter de trouver les coefficients de pond√©ration pour les fonctions de comparaison d‚Äôimages, de listes, de cat√©gories, de num√©ros et de descriptions. Cela permettrait √©galement de minimiser l‚Äôimpact des saisies incorrectes.\nAnalyse des photos : Il serait possible d‚Äôexploiter les informations utiles contenues dans les images, mais cela d√©pendrait de la qualit√© et des conditions de prise de vue des photos. (par exemple, la luminosit√© lors de la capture de l‚Äôext√©rieur du b√¢timent le matin)\nAnalyse G√©o-spatiale : Int√©grer des donn√©es g√©ospatiales pour affiner les regroupements en fonction de la localisation.\nCorrection des mauvaises saisies (typographie, fuzzywuzzy) : Mettre en place des m√©canismes de correction automatique pour les saisies incorrectes."
  },
  {
    "objectID": "backup/markdown_generator/OrcidToBib.html",
    "href": "backup/markdown_generator/OrcidToBib.html",
    "title": "",
    "section": "",
    "text": "orcid = '0000-0000-0000-0000' # Fill your orcid here\n\n\nimport requests\n\nWe use the /works api to list all works related to the orcid. This gives a summary of all works, so citation information is not included. We collect the put-code of all works to retrieve the citation information later.\n\nresponse = requests.get('https://pub.orcid.org/v3.0/{}/works'.format(orcid),\n                        headers={\"Accept\": \"application/orcid+json\" })\nrecord = response.json()\n\n\nput_codes = []\nfor work in record['group']:\n    put_code = work['work-summary'][0]['put-code']\n    put_codes.append(put_code)\nput_code = put_codes[0]\n\nWe use the /&lt;orcid&gt;/work/&lt;put-code&gt; endpoint to retrieve the citation information for each record.\n\ncitations = []\nfor put_code in put_codes:\n    response = requests.get('https://pub.orcid.org/v3.0/{}/work/{}'.format(orcid, put_code),\n                            headers={\"Accept\": \"application/orcid+json\" })\n    work = response.json()\n    if work['citation'] is not None:\n        citations.append(work['citation']['citation-value'])\n\n\nwith open('output.bib', 'w') as bibfile:\n    for citation in citations:\n        bibfile.write(citation)\n        bibfile.write('\\n')"
  },
  {
    "objectID": "backup/markdown_generator/PubsFromBib.html",
    "href": "backup/markdown_generator/PubsFromBib.html",
    "title": "Publications markdown generator for academicpages",
    "section": "",
    "text": "Takes a set of bibtex of publications and converts them for use with academicpages.github.io. This is an interactive Jupyter notebook (see more info here).\nThe core python code is also in pubsFromBibs.py. Run either from the markdown_generator folder after replacing updating the publist dictionary with: * bib file names * specific venue keys based on your bib file preferences * any specific pre-text for specific files * Collection Name (future feature)\nTODO: Make this work with other databases of citations, TODO: Merge this with the existing TSV parsing solution\n\nfrom pybtex.database.input import bibtex\nimport pybtex.database.input.bibtex \nfrom time import strptime\nimport string\nimport html\nimport os\nimport re\n\n\n#todo: incorporate different collection types rather than a catch all publications, requires other changes to template\npublist = {\n    \"proceeding\": {\n        \"file\" : \"proceedings.bib\",\n        \"venuekey\": \"booktitle\",\n        \"venue-pretext\": \"In the proceedings of \",\n        \"collection\" : {\"name\":\"publications\",\n                        \"permalink\":\"/publication/\"}\n        \n    },\n    \"journal\":{\n        \"file\": \"pubs.bib\",\n        \"venuekey\" : \"journal\",\n        \"venue-pretext\" : \"\",\n        \"collection\" : {\"name\":\"publications\",\n                        \"permalink\":\"/publication/\"}\n    } \n}\n\n\nhtml_escape_table = {\n    \"&\": \"&amp;\",\n    '\"': \"&quot;\",\n    \"'\": \"&apos;\"\n    }\n\ndef html_escape(text):\n    \"\"\"Produce entities within text.\"\"\"\n    return \"\".join(html_escape_table.get(c,c) for c in text)\n\n\nfor pubsource in publist:\n    parser = bibtex.Parser()\n    bibdata = parser.parse_file(publist[pubsource][\"file\"])\n\n    #loop through the individual references in a given bibtex file\n    for bib_id in bibdata.entries:\n        #reset default date\n        pub_year = \"1900\"\n        pub_month = \"01\"\n        pub_day = \"01\"\n        \n        b = bibdata.entries[bib_id].fields\n        \n        try:\n            pub_year = f'{b[\"year\"]}'\n\n            #todo: this hack for month and day needs some cleanup\n            if \"month\" in b.keys(): \n                if(len(b[\"month\"])&lt;3):\n                    pub_month = \"0\"+b[\"month\"]\n                    pub_month = pub_month[-2:]\n                elif(b[\"month\"] not in range(12)):\n                    tmnth = strptime(b[\"month\"][:3],'%b').tm_mon   \n                    pub_month = \"{:02d}\".format(tmnth) \n                else:\n                    pub_month = str(b[\"month\"])\n            if \"day\" in b.keys(): \n                pub_day = str(b[\"day\"])\n\n                \n            pub_date = pub_year+\"-\"+pub_month+\"-\"+pub_day\n            \n            #strip out {} as needed (some bibtex entries that maintain formatting)\n            clean_title = b[\"title\"].replace(\"{\", \"\").replace(\"}\",\"\").replace(\"\\\\\",\"\").replace(\" \",\"-\")    \n\n            url_slug = re.sub(\"\\\\[.*\\\\]|[^a-zA-Z0-9_-]\", \"\", clean_title)\n            url_slug = url_slug.replace(\"--\",\"-\")\n\n            md_filename = (str(pub_date) + \"-\" + url_slug + \".md\").replace(\"--\",\"-\")\n            html_filename = (str(pub_date) + \"-\" + url_slug).replace(\"--\",\"-\")\n\n            #Build Citation from text\n            citation = \"\"\n\n            #citation authors - todo - add highlighting for primary author?\n            for author in bibdata.entries[bib_id].persons[\"author\"]:\n                citation = citation+\" \"+author.first_names[0]+\" \"+author.last_names[0]+\", \"\n\n            #citation title\n            citation = citation + \"\\\"\" + html_escape(b[\"title\"].replace(\"{\", \"\").replace(\"}\",\"\").replace(\"\\\\\",\"\")) + \".\\\"\"\n\n            #add venue logic depending on citation type\n            venue = publist[pubsource][\"venue-pretext\"]+b[publist[pubsource][\"venuekey\"]].replace(\"{\", \"\").replace(\"}\",\"\").replace(\"\\\\\",\"\")\n\n            citation = citation + \" \" + html_escape(venue)\n            citation = citation + \", \" + pub_year + \".\"\n\n            \n            ## YAML variables\n            md = \"---\\ntitle: \\\"\"   + html_escape(b[\"title\"].replace(\"{\", \"\").replace(\"}\",\"\").replace(\"\\\\\",\"\")) + '\"\\n'\n            \n            md += \"\"\"collection: \"\"\" +  publist[pubsource][\"collection\"][\"name\"]\n\n            md += \"\"\"\\npermalink: \"\"\" + publist[pubsource][\"collection\"][\"permalink\"]  + html_filename\n            \n            note = False\n            if \"note\" in b.keys():\n                if len(str(b[\"note\"])) &gt; 5:\n                    md += \"\\nexcerpt: '\" + html_escape(b[\"note\"]) + \"'\"\n                    note = True\n\n            md += \"\\ndate: \" + str(pub_date) \n\n            md += \"\\nvenue: '\" + html_escape(venue) + \"'\"\n            \n            url = False\n            if \"url\" in b.keys():\n                if len(str(b[\"url\"])) &gt; 5:\n                    md += \"\\npaperurl: '\" + b[\"url\"] + \"'\"\n                    url = True\n\n            md += \"\\ncitation: '\" + html_escape(citation) + \"'\"\n\n            md += \"\\n---\"\n\n            \n            ## Markdown description for individual page\n            if note:\n                md += \"\\n\" + html_escape(b[\"note\"]) + \"\\n\"\n\n            if url:\n                md += \"\\n[Access paper here](\" + b[\"url\"] + \"){:target=\\\"_blank\\\"}\\n\" \n            else:\n                md += \"\\nUse [Google Scholar](https://scholar.google.com/scholar?q=\"+html.escape(clean_title.replace(\"-\",\"+\"))+\"){:target=\\\"_blank\\\"} for full citation\"\n\n            md_filename = os.path.basename(md_filename)\n\n            with open(\"../_publications/\" + md_filename, 'w', encoding=\"utf-8\") as f:\n                f.write(md)\n            print(f'SUCESSFULLY PARSED {bib_id}: \\\"', b[\"title\"][:60],\"...\"*(len(b['title'])&gt;60),\"\\\"\")\n        # field may not exist for a reference\n        except KeyError as e:\n            print(f'WARNING Missing Expected Field {e} from entry {bib_id}: \\\"', b[\"title\"][:30],\"...\"*(len(b['title'])&gt;30),\"\\\"\")\n            continue"
  },
  {
    "objectID": "old/index-bck1.html",
    "href": "old/index-bck1.html",
    "title": "My Academic Page",
    "section": "",
    "text": "Research Publications Teaching Contact"
  },
  {
    "objectID": "old/index-bck1.html#research",
    "href": "old/index-bck1.html#research",
    "title": "My Academic Page",
    "section": "Research",
    "text": "Research\nMy work explores neural networks, reproducibility, and structured learning‚Ä¶\n\n\n\n\n\n\nTipPublications\n\n\n\n\n\n\nDoe, J. (2025). Everforest architectures and interpretability.\n\nDoe, J. (2024). Reproducible MultiH5Datasets in AI research."
  },
  {
    "objectID": "old/index-bck1.html#teaching",
    "href": "old/index-bck1.html#teaching",
    "title": "My Academic Page",
    "section": "Teaching",
    "text": "Teaching\nI teach graduate-level courses on deep learning and model interpretability."
  },
  {
    "objectID": "old/index-bck1.html#contact",
    "href": "old/index-bck1.html#contact",
    "title": "My Academic Page",
    "section": "Contact",
    "text": "Contact\nüìß you@example.com\nüåê yourwebsite.com"
  }
]